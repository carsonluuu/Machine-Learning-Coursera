Part 1: Loading and Visualizing Data
     第一部分加载数据，并随机抽取前100个手写数字进行显示。ex3data1.mat这个文件里有5000个样本，每个样本是20*20的手写数字图片，所以是400维。X的Size为5000*400，而 y是label，大小为5000*1。数字10用0表示。


Part 2: Vectorize Logistic Regression
   第二部分是向量化逻辑回归。需要得到一个多类逻辑回归的分类器，用一个all_theta矩阵表示，第i行表示对label i的分类。all_theta是10*401的矩阵，调用误差函数和最小最优化函数可以求得每一行的最优参数值(使得每一行平方误差函数最小，也就得到了每一类的最优分类器)。

Part 3: Predict for One-Vs-All
   对整个数据5000*401 乘以训练好的分类矩阵的转置401*10 得到5000*10的数据，每一行代表一个样本，对一行来说，哪一列的样本经过sigmoid函数值最大，就判定为那个数字。

   其实这第一类对分类器的思路有点问题不知道大家发现没？他用整个数据集去训练，又用整个数据集来测试，这能说明什么？我觉得这并不能说明什么。我觉得用交叉验证的方法来得到一个准确度比实验结果给出的95%这个数值可靠一点。


――――――――――――Neural Networks――――――――――

   这一部分是用神经网络的强大来预测数字。数据集并没有改变，而层与层之间的权值矩阵是提前训练好的。

Part 1: Loading and Visualizing Data
    跟多类分类器并没有什么不同，还是读取数据，随机取前100数字显示。

Part 2: Loading Pameters
    加载权值矩阵数据，这里为2层，第一层是25*401 ，第二层是10*26。因为对数字进行分类，输出当然是10个了。

Part 3: Implement Predict
     predict函数执行方式就是一层一层的计算。

X = [ones(m, 1) X];      %加一列， 5000*401
a2 = sigmoid(X * Theta1');   % 第二层激活函数输出  5000*401  *401*25
a2 = [ones(m, 1) a2];        % 第二层加入b
a3 = sigmoid(a2 * Theta2');  
[aa,p] = max(a3,[],2);               % 返回每行最大值的索引位置，也就是预测的数字
       
        最后a3算出来就是5000*10的矩阵，同理，哪个值大就是哪个，第一个样本计算如下:
      0.0001    0.0017    0.0025    0.0000    0.0094    0.0040    0.0055    0.0004    0.0065    0.9957
      那第一个样本就是9了。
      用训练好的神经网络对样本里5000个样本数字进行判断，正确率为%97.52。